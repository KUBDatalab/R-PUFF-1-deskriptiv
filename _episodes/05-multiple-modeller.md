---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 05-multiple-modeller.md in _episodes_rmd/
title: "Multipel lineær regression - og polynomisk"
teaching: 10
exercises: 5
questions:
- "Hvordan fitter jeg lineære modeller af mere end en parameter?"
- "Hvordan fitter jeg polynomiske modeller?"
objectives:
- "FIXME"
keypoints:
- "FIXME"
source: Rmd
math: yes
---



# NB R-eksempler til modul 3 - ikke færdig!

Vi arbejder videre med fev datasættet. Først indlæser vi biblioteker:


~~~
library(tidyverse)
~~~
{: .language-r}

Og så indlæser vi datasættet:


~~~
Rows: 654 Columns: 6
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (6): Id, Age, FEV, Hgt, Sex, Smoke

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
~~~
{: .output}

~~~
fev <- read_csv("data/FEV.csv")
~~~
{: .language-r}

Vi kommer i øvelser til at arbejde med BONEDEN datasættet. Det læser vi også ind:


~~~
Rows: 41 Columns: 25
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (25): ID, age, zyg, ht1, wt1, tea1, cof1, alc1, cur1, men1, pyr1, ls1, f...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
~~~
{: .output}

~~~
boneden <- read_csv("data/BONEDEN.csv")
~~~
{: .language-r}


## Multipel lineær regression

Hvordan gør vi?


~~~
model1 <- lm(FEV ~Age +Hgt, data = fev)
~~~
{: .language-r}

Hvad hvis vi vil have en kategorisk variabel i modellen?


~~~
model2 <- lm(FEV ~Age + Hgt + factor(Sex), data = fev)
~~~
{: .language-r}

Hvis R skal kunne håndtere kategoriske variable korrekt, er vi nødt til at 
fortælle R at det er en kategorisk variabel. Det gør vi ved at pakke
variablen ind i `factor()`. 

Og hvis vi vil have to kategoriske variable i modellen?


~~~
model3 <- lm(FEV ~ Age + Hgt + factor(Sex) + factor(Smoke), data = fev)
~~~
{: .language-r}

Nu skal I selv! Vi starter med model1:

> ## Forudsigelse fra model 1
> 
> Forudsig det bedste bud på FEV for en 10-årig, der er 150 cm
> høj.
>
> Husk - data er i tommer, så divider højden med 2.54.
>
> > ## Løsningsforslag
> > 
> > Model1 fortæller os hvad koefficienterne er:
> > 
> > ~~~
> > summary(model1)
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > 
> > Call:
> > lm(formula = FEV ~ Age + Hgt, data = fev)
> > 
> > Residuals:
> >      Min       1Q   Median       3Q      Max 
> > -1.50533 -0.25657 -0.01184  0.24575  2.01914 
> > 
> > Coefficients:
> >              Estimate Std. Error t value Pr(>|t|)    
> > (Intercept) -4.610466   0.224271 -20.558  < 2e-16 ***
> > Age          0.054281   0.009106   5.961 4.11e-09 ***
> > Hgt          0.109712   0.004716  23.263  < 2e-16 ***
> > ---
> > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
> > 
> > Residual standard error: 0.4197 on 651 degrees of freedom
> > Multiple R-squared:  0.7664,	Adjusted R-squared:  0.7657 
> > F-statistic:  1068 on 2 and 651 DF,  p-value: < 2.2e-16
> > ~~~
> > {: .output}
> > 
> > Det giver denne model:
> >
> > $$FEV = -4.610466 + 0.054281*Age +  0.109712*Hgt$$
> > 
> > Højden beregner vi som 150/2.54 = 59.05512 i tommer.
> > 
> > Indsæt værdierne:
> > 
> > FEV = -4.610466 + 0.054281*10 +  0.109712*59.05512
> > 
> > eller: 2.411399
> >
> > Alternativ løsning:
> > 
> > 
> > ~~~
> > nye_data <- data.frame(Age = 10, Hgt = 59.05512)
> > 
> > predict(model1, newdata = nye_data)
> > summary(model1)
> > ~~~
> > {: .language-r}
> >
> {: .solution}
{: .challenge}



> ## Hvilket interval "stoler" vi på?
>
> I hvilket interval er vi 95% "sikre" på at FEV vil ligge for en
> 10-årig, 150 cm høj person?
>
> Hint: Konfidensintervaller
> 
> > ## Løsningsforslag
> > 
> > Vi skal bruge konfidensintervallerne på vores parametre:
> >
> > 
> > ~~~
> > confint(model1)
> > ~~~
> > {: .language-r}
> > 
> > Alternativ løsning:
> > 
> > predict(model1, newdata = nye_data, interval = "confidence")
> {: .solution}
{: .challenge}


> ## Giver det mening?
> 
> Hvad er vores bedste bud på FEV for en 0-årig, 150 cm høj person?
>
> Giver det mening?
> 
> > ## Løsningsforslag
> >
> > 
> {: .solution}
{: .challenge}

Og så fortsætter vi med model 2:

> ## Forudsigelse fra model 2
> 
> Hvad er modellens bedste bud på FEV for en 10-årig pige, der er 
> 150 cm høj?
>
> Hvad er modellens bedste bud på FEV for en 10-årig dreng, der
> er 150 cm høj?
>
> Hvad er forskellen? Siger modellen at den skal være der?
>
> > ## Løsningsforslag
> > 
> > 
> {: .solution}
{: .challenge}


> ## Hvilket interval "stoler" vi på?
>
> I hvilket interval er vi 95% "sikre" på at FEV er for denne
> pige?
> 
> 
> 
> > ## Løsningsforslag
> >
> >
> {: .solution}
{: .challenge}


Og til slut øvelser til model 3:


øvleser model 3

## Polynomiske modeller


Vi kommer til at lave en polynomisk model af FEV mod Hgt:


~~~
plot(fev$Hgt, fev$FEV)
~~~
{: .language-r}

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-unnamed-chunk-13-1.png" alt="plot of chunk unnamed-chunk-13" width="612" />
<p class="caption">plot of chunk unnamed-chunk-13</p>
</div>
Det kunne godt se ud som om FEV stiger lidt mere end bare lineært med højden.

Lineært fungerer OK:


~~~
linear_model <- lm(FEV ~ Hgt, data = fev)
summary(linear_model)
~~~
{: .language-r}



~~~

Call:
lm(formula = FEV ~ Hgt, data = fev)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.75167 -0.26619 -0.00401  0.24474  2.11936 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -5.432679   0.181460  -29.94   <2e-16 ***
Hgt          0.131976   0.002955   44.66   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4307 on 652 degrees of freedom
Multiple R-squared:  0.7537,	Adjusted R-squared:  0.7533 
F-statistic:  1995 on 1 and 652 DF,  p-value: < 2.2e-16
~~~
{: .output}

Vi laver en polynomisk model:


~~~
tredieordens_model <- lm(FEV ~ Hgt + I(Hgt^2) + I(Hgt^3), data = fev)
summary(tredieordens_model)
~~~
{: .language-r}



~~~

Call:
lm(formula = FEV ~ Hgt + I(Hgt^2) + I(Hgt^3), data = fev)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.79617 -0.22999  0.00229  0.21702  1.99786 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)
(Intercept)  4.569e-01  1.268e+01   0.036    0.971
Hgt          3.060e-02  6.361e-01   0.048    0.962
I(Hgt^2)    -1.522e-03  1.058e-02  -0.144    0.886
I(Hgt^3)     2.580e-05  5.829e-05   0.443    0.658

Residual standard error: 0.413 on 650 degrees of freedom
Multiple R-squared:  0.7742,	Adjusted R-squared:  0.7731 
F-statistic: 742.7 on 3 and 650 DF,  p-value: < 2.2e-16
~~~
{: .output}
Det var _ikke_ imponerende...


> ## I() funktionen
> 
> Hvis ikke vi pakker x^2 ind i en I() funktion, vil R forsøge at fortolke hvad
> x^2 betyder. R vil prøve at fortolke ^2 som et interaktionsled. Og det kan 
> føre til underlige resultater.
> 
> I() funktionen undertrykker Rs fortolkning af ting, og sender x^2 videre til
> lm() funktionen, "as is", som det er, uden at R prøver at gøre ting ved det.
> 
{: .callout}

Går det bedre med et andengradspolynomium?


~~~
andenordens_model <- lm(FEV ~ Hgt + I(Hgt^2), data = fev)
summary(andenordens_model)
~~~
{: .language-r}



~~~

Call:
lm(formula = FEV ~ Hgt + I(Hgt^2), data = fev)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.80103 -0.22928 -0.00255  0.21924  1.99699 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  6.0268779  1.5031843   4.009 6.79e-05 ***
Hgt         -0.2500488  0.0498553  -5.015 6.83e-07 ***
I(Hgt^2)     0.0031553  0.0004111   7.675 6.07e-14 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4127 on 651 degrees of freedom
Multiple R-squared:  0.7741,	Adjusted R-squared:  0.7734 
F-statistic:  1115 on 2 and 651 DF,  p-value: < 2.2e-16
~~~
{: .output}
_Meget_ bedre!

Hvordan ser det ud?


~~~
fev %>% ggplot(aes(x = FEV, y = Hgt)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "green", se = FALSE) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 2), colour = "blue", se = FALSE) +
  geom_smooth(method = "lm", formula = y ~ poly(x, 3), colour = "red", se = FALSE)
~~~
{: .language-r}



~~~
`geom_smooth()` using formula = 'y ~ x'
~~~
{: .output}

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-unnamed-chunk-17-1.png" alt="plot of chunk unnamed-chunk-17" width="612" />
<p class="caption">plot of chunk unnamed-chunk-17</p>
</div>








Vi har ikke taget stilling til hvilket datasæt der skal bruges som eksempel. 
Men kommer nok til at køre ca. samme model som øvelserne i forrige modul:

1. sådan gør du
2. gør det selv med et andet datasæt

Eksempelkode baserer sig aktuelt på mtcars datasættet.

Den multiple lineære regression opbygges på samme måde som den simple 
lineære regression. Formelnotationen er den samme. Nu er der blot flere
forklarende variable, der hver i sær tilføjes med et `+`:


~~~
model <- lm(mpg ~ cyl + hp + wt, data = mtcars)
~~~
{: .language-r}

Det direkte output af modellen er en smule mere kompliceret end vi har set før -
der er flere variable, og derfor flere koefficienter:


~~~
model
~~~
{: .language-r}



~~~

Call:
lm(formula = mpg ~ cyl + hp + wt, data = mtcars)

Coefficients:
(Intercept)          cyl           hp           wt  
   38.75179     -0.94162     -0.01804     -3.16697  
~~~
{: .output}
Dette kan vi tolke som, at `mpg`, miles pr gallon kan beskrives som:

$$ mpg = 38.75179 - 0.94162 * cyl - 0.01804 * hp - 3.16697 * wt$$ 
Jo flere hestekræfter, jo dårligere brændstoføkonomi. Jo tungere bil, jo dårligere
brændstoføkonomi. Men også dårligere brændstoføkonomi når bilen har flere cylindre.

Antallet af cylindre er teknisk set en kategorisk værdi. Bilmotorer kan have 
4, 6 eller 8 cylindre. Der er langt mellem de forbrændingsmotorer der har 5 cylindre.
Men der er ingen der har 4,3 cylindre. Der er også en sammenhæng mellem motorens
størrelse og antallet af cylindre. Og derfor "blander" vi de to parametre, cyl og hp.

Det er dårlig praksis...

Det mere detaljerede output af modellen får vi med `summary()` funktionen:


~~~
summary(model)
~~~
{: .language-r}



~~~

Call:
lm(formula = mpg ~ cyl + hp + wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.9290 -1.5598 -0.5311  1.1850  5.8986 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 38.75179    1.78686  21.687  < 2e-16 ***
cyl         -0.94162    0.55092  -1.709 0.098480 .  
hp          -0.01804    0.01188  -1.519 0.140015    
wt          -3.16697    0.74058  -4.276 0.000199 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.512 on 28 degrees of freedom
Multiple R-squared:  0.8431,	Adjusted R-squared:  0.8263 
F-statistic: 50.17 on 3 and 28 DF,  p-value: 2.184e-11
~~~
{: .output}
Det er også lidt længere end vi ellers har set - for der er flere 
parametre at estimere.


~~~
confint(model)
~~~
{: .language-r}



~~~
                 2.5 %       97.5 %
(Intercept) 35.0915623 42.412012412
cyl         -2.0701179  0.186884238
hp          -0.0423655  0.006289293
wt          -4.6839740 -1.649972191
~~~
{: .output}
Er det en god model? Standard plotte funktionen giver os fire plots, der kan
bruges til at vurdere hvor god den er.

~~~
plot(model)
~~~
{: .language-r}

<div class="figure" style="text-align: center">
<img src="../fig/rmd-05-unnamed-chunk-22-1.png" alt="plot of chunk unnamed-chunk-22" width="612" />
<p class="caption">plot of chunk unnamed-chunk-22</p>
</div><div class="figure" style="text-align: center">
<img src="../fig/rmd-05-unnamed-chunk-22-2.png" alt="plot of chunk unnamed-chunk-22" width="612" />
<p class="caption">plot of chunk unnamed-chunk-22</p>
</div><div class="figure" style="text-align: center">
<img src="../fig/rmd-05-unnamed-chunk-22-3.png" alt="plot of chunk unnamed-chunk-22" width="612" />
<p class="caption">plot of chunk unnamed-chunk-22</p>
</div><div class="figure" style="text-align: center">
<img src="../fig/rmd-05-unnamed-chunk-22-4.png" alt="plot of chunk unnamed-chunk-22" width="612" />
<p class="caption">plot of chunk unnamed-chunk-22</p>
</div>
Vi får fire plots, der alle handler om residualerne. En vigtig forudsætning for
den lineære regression er netop at residualerne er normalfordelte og tilfældige.

*Residuals vs Fitted* giver os residualerne af de fittede værdier. Altså, hvis 
cyl, hp og wt er hvad de nu er, hvor meget skulle mpg så være? Og hvad er mpg i
virkeligheden? De bør ligge helt tilfældigt. Hvis det ser ud som om der er 
mønstre, er der et eller andet vores model ikke har fanget.

*Q-Q residuals* er en visuel test af om resdiualerne er normalfordelte. Punkterne
bør ligge langs den lige linie. Det gør de sjældent helt.

*Scale-Location* viser os om spredningen af residualerne ændrer sig systematisk
med de fittede værdier. De skal helst ligge tilfældigt omkring en ret, horisontal
linie med værdien 0.

*Residuals vs Leverage* hjælper os til at identificere datapunkter, der har 
uforholdsmæssig indflydelse, "leverage" på modellen. Man kan forstå det som "hvis
dette punkt blev pillet ud af datasættet, ville parametrene i modellen så ændre sig
markant". Cook's distance som man kan se i plottet, giver os et bud på om 
et punkt ændrer parametrene markant.




{% include links.md %}
